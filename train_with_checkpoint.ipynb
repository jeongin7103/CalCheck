{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1rABpT7ZPHzsqiJPANKQH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeongin7103/CalCheck/blob/main/train_with_checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBGWkCTTMwlI",
        "outputId": "38255802-e1d1-4434-edf8-46f4b7aa63ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/SSD_detection')"
      ],
      "metadata": {
        "id": "S6sD3Vy5NtLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "from model import SSD300, MultiBoxLoss\n",
        "import torch.utils.data\n",
        "from utils import *\n",
        "from datasets import CustomDataset"
      ],
      "metadata": {
        "id": "ZGQ9zmjgNtNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = '/content/drive/MyDrive/SSD_detection/train'"
      ],
      "metadata": {
        "id": "rtpoQ8SENtQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global start_epoch, label_map, epoch, checkpoint, decay_lr_at"
      ],
      "metadata": {
        "id": "IcseE_KUNtSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "n_classes = len(label_map)\n",
        "print(n_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Learning parameters\n",
        "checkpoint = \"/content/drive/MyDrive/SSD_detection/checkpoints/checkpoint_ssd300_epoch_16.pth.tar\"  # checkpoint 경로 넣어주기\n",
        "batch_size = 4\n",
        "iterations = 3\n",
        "# workers = 4\n",
        "print_freq = 100\n",
        "lr = 1e-3\n",
        "decay_lr_at = [80000, 100000]\n",
        "decay_lr_to = 0.1  # decay learning rate to this fraction of the existing learning rate\n",
        "momentum = 0.9  # momentum\n",
        "weight_decay = 5e-4  # weight decay\n",
        "grad_clip = None\n",
        "cudnn.benchmark = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf8N_kRpNy9V",
        "outputId": "112ba12d-dec8-4477-81b2-50b5163c33e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()  # training mode enables dropout\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
        "    data_time = AverageMeter()  # data loading time\n",
        "    losses = AverageMeter()  # loss\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Batches\n",
        "    for i, (images, boxes, labels) in enumerate(train_loader):\n",
        "        # images: (N, 3, 300, 300)\n",
        "        data_time.update(time.time() - start)\n",
        "\n",
        "        # Move to default device\n",
        "        # images\n",
        "        images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
        "        boxes = [b.to(device) for b in boxes]\n",
        "        labels = [l.to(device) for l in labels]\n",
        "\n",
        "        # Forward prop.\n",
        "        # 여기서 model.py의 forward 함수의 인자로 넣어줄 images 가 전달된다.\n",
        "        predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
        "\n",
        "        # Loss\n",
        "\n",
        "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
        "\n",
        "        # Backward prop.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients, if necessary\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(optimizer, grad_clip)\n",
        "\n",
        "        # Update model\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print status\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
        "                                                                  batch_time=batch_time,\n",
        "                                                                  data_time=data_time, loss=losses))\n",
        "    del predicted_locs, predicted_scores, images, boxes, labels  # free some memory since their histories may be stored"
      ],
      "metadata": {
        "id": "LfOrae0ANy_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if checkpoint is None:\n",
        "    start_epoch = 0\n",
        "    model = SSD300(n_classes=21)\n",
        "    biases = list()\n",
        "    not_biases = list()\n",
        "    for param_name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if param_name.endswith('.bias'):\n",
        "                biases.append(param)\n",
        "            else:\n",
        "                not_biases.append(param)\n",
        "    optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
        "                                lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "else:\n",
        "    # checkpoint = torch.load(checkpoint, map_location=torch.device('cpu'))\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print('\\nLoaded checkpoint from epoch %d.\\n' % start_epoch)\n",
        "    model = checkpoint['model']\n",
        "    optimizer = checkpoint['optimizer']"
      ],
      "metadata": {
        "id": "U5umo6J9NzCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40773a3b-52c0-445c-c7cb-f86c852f19fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded checkpoint from epoch 17.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move to default device\n",
        "model = model.to(device)\n",
        "\n",
        "# loss 함수 지정\n",
        "criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n",
        "\n",
        "# Custom dataloaders\n",
        "train_dataset = CustomDataset(data_folder, split='train')\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                           collate_fn=train_dataset.collate_fn,\n",
        "                                           pin_memory=True)  # note that we're passing the collate function here"
      ],
      "metadata": {
        "id": "peoeZ6A0NzEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "# decay_lr_at = [it // (len(train_dataset) // 32) for it in decay_lr_at]\n",
        "decay_lr_at = [10,18]\n",
        "print(epochs)\n",
        "print(decay_lr_at)"
      ],
      "metadata": {
        "id": "WRAVOo5oN5Ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a205f5-b7a1-42d6-9a34-c2776296c9f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "[10, 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    # Decay learning rate at particular epochs\n",
        "    if epoch in decay_lr_at:\n",
        "        adjust_learning_rate(optimizer, decay_lr_to)\n",
        "\n",
        "    # One epoch's train, train 함수로 학습 진행\n",
        "    train(train_loader=train_loader, model=model, criterion=criterion,\n",
        "          optimizer=optimizer,\n",
        "          epoch=epoch)\n",
        "    # Save checkpoint\n",
        "    save_checkpoint(epoch, model, optimizer)"
      ],
      "metadata": {
        "id": "CBYvKk3qN5L_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031ce47d-e8a2-48b7-a2fe-1561fabbd0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [17][0/2000]\tBatch Time 11.134 (11.134)\tData Time 4.621 (4.621)\tLoss 1.6977 (1.6977)\t\n",
            "Epoch: [17][100/2000]\tBatch Time 3.068 (3.361)\tData Time 2.818 (3.051)\tLoss 2.2388 (2.1518)\t\n",
            "Epoch: [17][200/2000]\tBatch Time 3.138 (3.301)\tData Time 2.887 (3.022)\tLoss 2.0402 (2.1335)\t\n",
            "Epoch: [17][300/2000]\tBatch Time 3.129 (3.266)\tData Time 2.877 (2.998)\tLoss 0.9781 (2.1136)\t\n",
            "Epoch: [17][400/2000]\tBatch Time 3.246 (3.270)\tData Time 3.019 (3.006)\tLoss 1.1402 (2.0933)\t\n",
            "Epoch: [17][500/2000]\tBatch Time 3.446 (3.263)\tData Time 3.212 (3.004)\tLoss 1.2110 (2.0769)\t\n",
            "Epoch: [17][600/2000]\tBatch Time 3.213 (3.257)\tData Time 2.979 (2.999)\tLoss 2.5041 (2.0551)\t\n",
            "Epoch: [17][700/2000]\tBatch Time 3.211 (3.257)\tData Time 2.959 (3.001)\tLoss 2.7339 (2.0424)\t\n",
            "Epoch: [17][800/2000]\tBatch Time 3.403 (3.255)\tData Time 3.172 (3.000)\tLoss 1.8217 (2.0508)\t\n",
            "Epoch: [17][900/2000]\tBatch Time 3.571 (3.256)\tData Time 3.316 (3.003)\tLoss 1.2730 (2.0459)\t\n",
            "Epoch: [17][1000/2000]\tBatch Time 3.262 (3.254)\tData Time 3.027 (3.001)\tLoss 1.8691 (2.0370)\t\n",
            "Epoch: [17][1100/2000]\tBatch Time 3.598 (3.253)\tData Time 3.335 (3.001)\tLoss 1.7580 (2.0375)\t\n",
            "Epoch: [17][1200/2000]\tBatch Time 3.386 (3.255)\tData Time 3.142 (3.003)\tLoss 1.0751 (2.0316)\t\n",
            "Epoch: [17][1300/2000]\tBatch Time 3.064 (3.256)\tData Time 2.815 (3.004)\tLoss 1.2244 (2.0223)\t\n",
            "Epoch: [17][1400/2000]\tBatch Time 3.103 (3.258)\tData Time 2.855 (3.006)\tLoss 1.6807 (2.0211)\t\n",
            "Epoch: [17][1500/2000]\tBatch Time 2.961 (3.255)\tData Time 2.694 (3.004)\tLoss 1.2622 (2.0085)\t\n",
            "Epoch: [17][1600/2000]\tBatch Time 3.256 (3.251)\tData Time 3.020 (3.000)\tLoss 2.2525 (2.0166)\t\n",
            "Epoch: [17][1700/2000]\tBatch Time 3.072 (3.253)\tData Time 2.839 (3.002)\tLoss 1.4005 (2.0076)\t\n",
            "Epoch: [17][1800/2000]\tBatch Time 3.641 (3.252)\tData Time 3.414 (3.002)\tLoss 2.0361 (2.0040)\t\n",
            "Epoch: [17][1900/2000]\tBatch Time 2.935 (3.253)\tData Time 2.676 (3.003)\tLoss 0.9831 (1.9998)\t\n",
            "DECAYING learning rate.\n",
            " The new LR is 0.000010\n",
            "\n",
            "Epoch: [18][0/2000]\tBatch Time 0.412 (0.412)\tData Time 0.255 (0.255)\tLoss 2.0333 (2.0333)\t\n",
            "Epoch: [18][100/2000]\tBatch Time 0.255 (0.281)\tData Time 0.094 (0.119)\tLoss 2.0538 (2.0001)\t\n",
            "Epoch: [18][200/2000]\tBatch Time 0.291 (0.275)\tData Time 0.133 (0.115)\tLoss 3.7589 (2.0466)\t\n",
            "Epoch: [18][300/2000]\tBatch Time 0.246 (0.273)\tData Time 0.088 (0.114)\tLoss 0.7804 (1.9894)\t\n",
            "Epoch: [18][400/2000]\tBatch Time 0.218 (0.274)\tData Time 0.056 (0.114)\tLoss 4.3701 (1.9838)\t\n",
            "Epoch: [18][500/2000]\tBatch Time 0.329 (0.274)\tData Time 0.170 (0.115)\tLoss 1.7118 (1.9799)\t\n",
            "Epoch: [18][600/2000]\tBatch Time 0.329 (0.273)\tData Time 0.170 (0.114)\tLoss 1.7998 (1.9704)\t\n",
            "Epoch: [18][700/2000]\tBatch Time 0.329 (0.273)\tData Time 0.169 (0.113)\tLoss 2.8181 (1.9731)\t\n",
            "Epoch: [18][800/2000]\tBatch Time 0.231 (0.272)\tData Time 0.067 (0.112)\tLoss 0.8865 (1.9710)\t\n",
            "Epoch: [18][900/2000]\tBatch Time 0.229 (0.272)\tData Time 0.068 (0.113)\tLoss 1.2797 (1.9734)\t\n",
            "Epoch: [18][1000/2000]\tBatch Time 0.230 (0.272)\tData Time 0.070 (0.113)\tLoss 1.8137 (1.9769)\t\n",
            "Epoch: [18][1100/2000]\tBatch Time 0.294 (0.271)\tData Time 0.136 (0.112)\tLoss 1.2827 (1.9697)\t\n",
            "Epoch: [18][1200/2000]\tBatch Time 0.230 (0.271)\tData Time 0.069 (0.112)\tLoss 1.9020 (1.9769)\t\n",
            "Epoch: [18][1300/2000]\tBatch Time 0.243 (0.271)\tData Time 0.085 (0.112)\tLoss 2.7417 (1.9843)\t\n",
            "Epoch: [18][1400/2000]\tBatch Time 0.215 (0.271)\tData Time 0.056 (0.112)\tLoss 1.9409 (1.9790)\t\n",
            "Epoch: [18][1500/2000]\tBatch Time 0.350 (0.271)\tData Time 0.190 (0.112)\tLoss 4.3169 (1.9805)\t\n",
            "Epoch: [18][1600/2000]\tBatch Time 0.243 (0.271)\tData Time 0.083 (0.111)\tLoss 1.4405 (1.9778)\t\n",
            "Epoch: [18][1700/2000]\tBatch Time 0.269 (0.271)\tData Time 0.110 (0.112)\tLoss 2.6994 (1.9874)\t\n",
            "Epoch: [18][1800/2000]\tBatch Time 0.240 (0.271)\tData Time 0.080 (0.111)\tLoss 1.5460 (1.9850)\t\n",
            "Epoch: [18][1900/2000]\tBatch Time 0.241 (0.270)\tData Time 0.083 (0.111)\tLoss 1.0117 (1.9809)\t\n",
            "Epoch: [19][0/2000]\tBatch Time 0.234 (0.234)\tData Time 0.075 (0.075)\tLoss 0.9008 (0.9008)\t\n",
            "Epoch: [19][100/2000]\tBatch Time 0.320 (0.276)\tData Time 0.161 (0.117)\tLoss 0.9733 (1.9603)\t\n",
            "Epoch: [19][200/2000]\tBatch Time 0.257 (0.272)\tData Time 0.096 (0.112)\tLoss 1.7797 (1.9650)\t\n",
            "Epoch: [19][300/2000]\tBatch Time 0.386 (0.271)\tData Time 0.226 (0.112)\tLoss 1.4283 (1.9409)\t\n",
            "Epoch: [19][400/2000]\tBatch Time 0.277 (0.273)\tData Time 0.114 (0.114)\tLoss 2.1085 (1.9690)\t\n",
            "Epoch: [19][500/2000]\tBatch Time 0.255 (0.272)\tData Time 0.097 (0.113)\tLoss 0.8288 (1.9493)\t\n",
            "Epoch: [19][600/2000]\tBatch Time 0.215 (0.272)\tData Time 0.055 (0.112)\tLoss 1.5730 (1.9283)\t\n",
            "Epoch: [19][700/2000]\tBatch Time 0.260 (0.272)\tData Time 0.101 (0.113)\tLoss 2.7340 (1.9574)\t\n",
            "Epoch: [19][800/2000]\tBatch Time 0.253 (0.273)\tData Time 0.097 (0.113)\tLoss 1.9751 (1.9680)\t\n",
            "Epoch: [19][900/2000]\tBatch Time 0.219 (0.272)\tData Time 0.059 (0.113)\tLoss 1.3456 (1.9797)\t\n",
            "Epoch: [19][1000/2000]\tBatch Time 0.213 (0.272)\tData Time 0.053 (0.113)\tLoss 1.3916 (1.9836)\t\n",
            "Epoch: [19][1100/2000]\tBatch Time 0.223 (0.272)\tData Time 0.063 (0.113)\tLoss 0.5950 (1.9804)\t\n",
            "Epoch: [19][1200/2000]\tBatch Time 0.246 (0.271)\tData Time 0.087 (0.112)\tLoss 4.2071 (1.9806)\t\n",
            "Epoch: [19][1300/2000]\tBatch Time 0.268 (0.272)\tData Time 0.111 (0.112)\tLoss 1.4167 (1.9808)\t\n",
            "Epoch: [19][1400/2000]\tBatch Time 0.261 (0.271)\tData Time 0.105 (0.112)\tLoss 3.4760 (1.9771)\t\n",
            "Epoch: [19][1500/2000]\tBatch Time 0.233 (0.271)\tData Time 0.074 (0.112)\tLoss 1.1626 (1.9746)\t\n",
            "Epoch: [19][1600/2000]\tBatch Time 0.279 (0.271)\tData Time 0.120 (0.112)\tLoss 2.2463 (1.9695)\t\n",
            "Epoch: [19][1700/2000]\tBatch Time 0.388 (0.271)\tData Time 0.229 (0.112)\tLoss 2.6973 (1.9707)\t\n",
            "Epoch: [19][1800/2000]\tBatch Time 0.320 (0.271)\tData Time 0.159 (0.112)\tLoss 2.5015 (1.9728)\t\n",
            "Epoch: [19][1900/2000]\tBatch Time 0.354 (0.271)\tData Time 0.193 (0.112)\tLoss 1.2325 (1.9716)\t\n"
          ]
        }
      ]
    }
  ]
}